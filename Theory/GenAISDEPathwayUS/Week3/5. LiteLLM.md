# ü§ñ LiteLLM: Unified API for All Major LLMs

**LiteLLM** is an open-source **LLM API gateway** and **Python SDK** that provides a **unified, OpenAI-compatible interface** to interact with **over 100+ LLMs** from providers like OpenAI, Anthropic, Mistral, Google, Cohere, Hugging Face, and more.

---

## üß© Key Features

| Feature | Description |
|--------|-------------|
| **Unified API** | Offers a standard OpenAI-style API (`/v1/chat/completions`) for all supported models |
| **Model Switching** | Swap between GPT-4, Claude, Mistral, Gemini, etc., with just a model name change |
| **Logging & Monitoring** | Built-in support for cost tracking, latency, rate limits, and usage logs |
| **Fallbacks & Routing** | Automatically reroutes requests if a provider fails or exceeds limits |
| **Embeddings & Streaming** | Supports embeddings and streaming output in a consistent format |
| **Gateway Mode** | Can run as a local proxy server emulating OpenAI API endpoints |

---

## ‚úÖ When to Use LiteLLM

Use LiteLLM if:

- You want to easily **switch LLM providers** without rewriting code
- You need **fine-grained control** over cost, rate limiting, or logs
- You're deploying an **enterprise system** that calls multiple models
- You want a **drop-in OpenAI replacement** for your app/server

---

## üîß Code Example: Basic Completion

```python
from litellm import completion

response = completion(
    model="openai/gpt-4o",  # Or "anthropic/claude-3-opus", "mistral/mistral-tiny"
    messages=[
        {"role": "user", "content": "Tell me a joke."}
    ]
)

print(response.choices[0].message.content)
```

# üß† How LiteLLM Standardizes LLM API Calls

**LiteLLM** offers a unified interface to over 100+ LLMs by standardizing their API schemas using the OpenAI chat/completions format.

---

## üîß 1. Unified API Schema

### ‚úÖ Input Format (OpenAI-Compatible)

Regardless of the backend provider (e.g., OpenAI, Anthropic, Mistral), the format is:

```python
from litellm import completion

response = completion(
    model="anthropic/claude-3-opus",  # or openai/gpt-4, mistral/mistral-tiny
    messages=[
        {"role": "user", "content": "What is the capital of France?"}
    ]
)
print(response.choices[0].message.content)
```

### ‚úÖ Output Format

All responses are returned in the same format:

```json
{
  "id": "chatcmpl-xyz",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "The capital of France is Paris."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 7,
    "completion_tokens": 9,
    "total_tokens": 16
  }
}
```

### üîπ Contrast with LangChain

LangChain requires **different wrappers** for each provider:

```python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

# OpenAI GPT-4
llm_openai = ChatOpenAI(model="gpt-4o-mini")
response_openai = llm_openai.invoke("Write a haiku about the ocean")
print("LangChain OpenAI:", response_openai.content)

# Anthropic Claude
llm_claude = ChatAnthropic(model="claude-3-haiku")
response_claude = llm_claude.invoke("Write a haiku about the ocean")
print("LangChain Claude:", response_claude.content)
```
---

## üîÅ 2. Adapter Layer for Providers

Each provider has a dedicated adapter that:

- Translates OpenAI-style requests to provider-specific formats
- Normalizes responses back to OpenAI format
- Supports token counting, streaming, and more

Example: Claude requires a `prompt`, not `messages`.

```python
def convert_openai_to_anthropic(messages):
    prompt = "\n\n".join([f"{m['role'].capitalize()}: {m['content']}" for m in messages])
    return anthropic_client.completion(prompt=prompt, ...)
```

---

## üöø 3. Streaming Example

Streaming is consistent across providers using Python generators:

```python
from litellm import completion

response = completion(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Tell me a story."}],
    stream=True
)

for chunk in response:
    print(chunk.choices[0].delta.content or "", end="")
```

---

## üìä 4. Normalized Features

| Feature             | Description |
|---------------------|-------------|
| **Embeddings**      | Unified `embedding()` call |
| **Streaming**       | OpenAI-style streaming output |
| **Image input**     | Normalized format for GPT-4o, Gemini Vision |
| **Async support**   | All methods are async-compatible |
| **Token counting**  | Accurate and consistent |
| **Cost tracking**   | Standardized across providers |

---

## üì¶ 5. Proxy Server Mode

When run as a server, LiteLLM exposes:

- `/v1/chat/completions` (OpenAI-style)
- Accepts OpenAI requests
- Routes to the selected provider automatically
- Provides logging, rate limiting, fallback

---

## üß† Analogy

Like **Stripe** abstracts Visa/Mastercard/UPI under one API, **LiteLLM** abstracts GPT/Claude/Mistral under one LLM API.

---
