# ðŸ“š What is Haystack?

Haystack is an **open-source framework** developed by **deepset** that enables developers to build **production-ready LLM (Large Language Model) applications**, especially for **search**, **retrieval-augmented generation (RAG)**, and **question answering (QA)** over custom data sources.

---

## ðŸ” Core Purpose
Haystack provides tools to implement **RAG pipelines**:

1. **Retrieve** relevant context from your documents.
2. **Generate** answers using an LLM like OpenAI, Cohere, or HuggingFace models.

---

## âœ… Key Features

### 1. Modular Pipelines
Haystack introduces a pipeline abstraction composed of modular building blocks:

- **DocumentStore** (e.g., FAISS, Weaviate, Elasticsearch)
- **Retriever** (BM25, DenseRetriever)
- **Reader/Generator** (LLMs or extractive models)
- **PromptNode** (for custom prompt engineering)

### 2. Multimodal & Multilingual
- Supports PDFs, images, audio, tables
- 100+ languages supported

### 3. Built-in Evaluation Tools
- Automatic metrics like F1 and Exact Match
- Human-in-the-loop support

### 4. Deployment Support
- REST APIs (FastAPI)
- Streamlit UI
- Docker/Kubernetes-ready

### 5. PromptNode
A powerful wrapper around LLMs with templated prompting and chaining logic.

---

## ðŸ§± Sample Pipeline Structure

```text
[User Query]
    â†“
[Retriever (FAISS, Weaviate, etc.)]
    â†“
[PromptNode (e.g., OpenAI GPT)]
    â†“
[Generated Answer]
```

---

## ðŸ“¦ Common Use Cases

| Use Case                      | Description                                      |
|-------------------------------|--------------------------------------------------|
| Custom Chatbots               | Q&A over internal/company documents              |
| Legal/Medical Document Search | Retrieves contextually relevant answers          |
| Developer Assistants          | Answers from code, documentation, tickets       |
| Enterprise Search             | Semantic search for internal knowledge bases     |

---

## ðŸ”„ Comparison with Other Frameworks

| Feature                     | Haystack              | LangChain             | LlamaIndex            |
|----------------------------|------------------------|------------------------|------------------------|
| Primary Focus              | RAG, QA                | Agents, chaining       | Data connectors & RAG  |
| Pipeline Engine            | âœ… Yes                 | âœ… Yes                 | âš ï¸ Limited             |
| Built-in Eval              | âœ… Yes                 | âŒ Manual              | âš ï¸ Basic               |
| Prompt Engineering         | PromptNode             | Manual/templates       | Prompt Helper          |
| Deployment Ready           | FastAPI, Streamlit     | Requires manual setup  | Local/Lightweight only |

---
## ðŸ“Š Evaluation Metrics in Detail

### 1. Exact Match (EM)
Checks if the model's predicted answer exactly matches the expected answer after normalization (case-folding, removing punctuation).

**Example:**
- Prediction: "Paris"
- Ground Truth: "Paris"
- EM = 1.0

**Example:**
- Prediction: "The capital of France is Paris."
- Ground Truth: "Paris"
- EM = 0.0
---
### 2. F1 Score
Captures token-level overlap â€” the harmonic mean of precision and recall.

**Example:**
- Prediction: "The notice period is 30 days."
- Ground Truth: "30 days"
- Common tokens: ["30", "days"]
- Precision = 2/6, Recall = 2/2, F1 â‰ˆ 0.57

---
### 3. Cosine Similarity (Embedding-Based)
Converts both prediction and ground truth into vector embeddings (e.g., using sentence transformers) and computes the cosine similarity between them.

**Example:**
- Prediction: "Termination requires a 30-day notice."
- Ground Truth: "30 days notice period."
- Cosine Similarity â‰ˆ 0.89

---
### 4. BLEU (Bilingual Evaluation Understudy)
**BLEU** measures how close a machine-generated text is to one or more reference texts using **n-gram precision**.

#### ðŸ”¢ How BLEU Works
- Measures **n-gram overlap** (1-gram to 4-gram) between candidate and reference.
- Includes **clipping** to avoid over-counting words.
- Applies a **brevity penalty** for short outputs.

#### ðŸ§  Example
**Reference:** The cat is on the mat  
**Candidate:** The cat sat on the mat

- 1-gram precision = 5/6 = 0.83
- 2-gram precision = 3/5 = 0.6

```python
from nltk.translate.bleu_score import sentence_bleu

ref = [["the", "cat", "is", "on", "the", "mat"]]
cand = ["the", "cat", "sat", "on", "the", "mat"]
score = sentence_bleu(ref, cand)
```

#### âœ… Best For
- Machine translation
- Short, structured generation tasks

#### âš ï¸ Limitations
- Does not handle synonyms or paraphrases
- Precision-focused; not good for recall

---

### 5. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

**ROUGE** is a set of metrics that measure **recall-based n-gram overlap** or sequence similarity.

#### ðŸ” ROUGE Types

| Metric     | Description                              |
|------------|------------------------------------------|
| ROUGE-N    | N-gram recall (e.g., ROUGE-1, ROUGE-2)   |
| ROUGE-L    | Longest common subsequence (LCS)         |
| ROUGE-W    | Weighted LCS (penalizes gaps)            |
| ROUGE-S    | Skip-bigram                              |

#### ðŸ§  Example

**Reference:** The cat is on the mat  
**Candidate:** The cat sat on the mat

- ROUGE-1 Recall = 5/6 = 0.83  
- ROUGE-2 Recall = 3/5 = 0.6  
- ROUGE-L = LCS (The cat on the mat) â†’ Recall = 5/6

```python
from rouge_score import rouge_scorer
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
scorer.score("The cat is on the mat", "The cat sat on the mat")
```

#### âœ… Best For
- Summarization evaluation
- Tasks requiring coverage measurement

#### âš ï¸ Limitations
- Doesnâ€™t handle semantic meaning
- Over-relies on exact words

---

### 6. BERTScore

**BERTScore** compares two texts using **contextual embeddings** from BERT models. It evaluates **semantic similarity** rather than surface overlap.

#### ðŸ”¢ How It Works
1. Tokenize and embed both candidate and reference using BERT.
2. Compute cosine similarity between each token.
3. Calculate **Precision**, **Recall**, and **F1** scores.

#### ðŸ§  Example

**Reference:** The patient suffers from high blood pressure.  
**Prediction:** The patient has hypertension.

- BERTScore F1 â‰ˆ 0.85 (semantic match)

```python
from bert_score import score
pred = ["The patient has hypertension."]
ref = ["The patient suffers from high blood pressure."]
P, R, F1 = score(pred, ref, lang="en")
```

#### âœ… Best For
- Open-ended QA
- Paraphrase and semantic evaluation

#### âš ï¸ Limitations
- Slower than BLEU/ROUGE
- Requires GPU for larger models
- Less interpretable

---
### 7. Human Feedback

Users can rate answers 1â€“5 for relevance and correctness.

---
### 8. Retrieval Metrics (Hit@K, MRR, nDCG)
Evaluate the retriever:
- **Hit@K**: Was the relevant doc in top K?
- **MRR**: Mean Reciprocal Rank of the correct doc
- **nDCG**: Accounts for the order and relevance
---
# ðŸ› Real-World Example: â€œChat with Legal Documentsâ€ using Haystack

## ðŸŽ¯ Goal

Build a **semantic search + RAG app** where users can ask questions about legal contracts (PDFs), and the system returns accurate answers using **retrieval-augmented generation (RAG)** with LLMs.

---

## âœ… Functional Requirements

- Upload legal documents (PDFs)
- Ask questions about them
- Return accurate, grounded answers using RAG
- Simple web UI (Streamlit or REST API)
- Logging of user queries and responses
- Evaluation using standard metrics (F1, EM, BERTScore)
- Deployable on cloud or containerized infrastructure

---

## ðŸ§© Architecture Overview

```text
PDF Upload â†’ Text Chunking â†’ Embedding â†’ Vector Store (FAISS)
                      â†‘
User Query â†’ Embed â†’ Retriever â†’ Top-k Chunks
                                 â†“
                          PromptNode (LLM)
                                 â†“
                          Final Answer
```

---

## ðŸ”§ Technologies Used

| Component       | Tool                          |
|------------------|-------------------------------|
| Vector Store     | FAISS                         |
| Text Preprocessing | Haystackâ€™s PreProcessor      |
| Embedding Model  | `sentence-transformers/all-MiniLM-L6-v2` |
| LLM Generator    | OpenAI GPT (via PromptNode)   |
| App Layer        | Streamlit or FastAPI          |
| Logging          | Python logging                |
| Evaluation       | F1, Exact Match, BERTScore    |
| Deployment       | Docker, Streamlit Cloud       |

---

## ðŸš€ Sample Code Implementation

### 1. ðŸ“¦ Install dependencies

```bash
pip install farm-haystack[all] openai bert-score rouge-score
```

---

### 2. ðŸ“‚ Load and Index PDF Documents

```python
from haystack.nodes import PDFToTextConverter, PreProcessor
from haystack.document_stores import FAISSDocumentStore
from haystack.nodes import EmbeddingRetriever

converter = PDFToTextConverter(remove_numeric_tables=True)
doc = converter.convert(file_path="contract.pdf", meta=None)

preprocessor = PreProcessor(split_by="word", split_length=200, split_overlap=30)
docs = preprocessor.process([doc])

document_store = FAISSDocumentStore(embedding_dim=384, faiss_index_factory_str="Flat")
retriever = EmbeddingRetriever(
    document_store=document_store,
    embedding_model="sentence-transformers/all-MiniLM-L6-v2",
    use_gpu=True
)
document_store.write_documents(docs)
document_store.update_embeddings(retriever)
```

---

### 3. ðŸ¤– PromptNode for Answer Generation

```python
from haystack.nodes import PromptNode, PromptTemplate

prompt_node = PromptNode(
    model_name_or_path="gpt-4",
    api_key="your-openai-key",
    default_prompt_template=PromptTemplate(
        name="rag-default",
        prompt="Answer based on context:\n\n{join(documents)}\n\nQuestion: {query}\nAnswer:",
        input_variables=["documents", "query"]
    )
)
```

---

### 4. ðŸ” Create the Pipeline

```python
from haystack.pipelines import Pipeline

rag_pipeline = Pipeline()
rag_pipeline.add_node(component=retriever, name="Retriever", inputs=["Query"])
rag_pipeline.add_node(component=prompt_node, name="Generator", inputs=["Retriever"])
```

---

### 5. ðŸ§ª Evaluation Metrics

```python
from sklearn.metrics import f1_score
from bert_score import score as bert_score

def evaluate(predicted, ground_truth):
    predictions = [p.lower().strip() for p in predicted]
    references = [r.lower().strip() for r in ground_truth]
    
    preds = [set(p.split()) for p in predictions]
    refs = [set(r.split()) for r in references]
    f1s = [2 * len(p & r) / (len(p) + len(r)) if (len(p) + len(r)) > 0 else 0 for p, r in zip(preds, refs)]
    
    P, R, F1 = bert_score(predictions, references, lang="en")
    
    return {"token_f1": sum(f1s)/len(f1s), "bert_score": F1.mean().item()}
```

---

### 6. ðŸ“œ Logging Setup

```python
import logging

logging.basicConfig(filename="rag_queries.log", level=logging.INFO)

def log_query(query, answer):
    logging.info(f"User Query: {query} | Answer: {answer}")
```

---

### 7. ðŸ’» Streamlit App

```python
import streamlit as st

st.title("Chat with Legal Documents")
query = st.text_input("Ask a question:")
if query:
    result = rag_pipeline.run(query=query, params={"Retriever": {"top_k": 5}})
    answer = result["results"][0]
    st.write(answer)
    log_query(query, answer)
```

---

### 8. ðŸ“¦ Deployment via Docker

**Dockerfile**
```dockerfile
FROM python:3.10
WORKDIR /app
COPY . .
RUN pip install -r requirements.txt
CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.enableCORS=false"]
```

---

## ðŸ§¾ Summary

| Feature         | Status   |
|------------------|----------|
| RAG Pipeline     | âœ…        |
| Logging          | âœ…        |
| Evaluation       | âœ…        |
| Web Interface    | âœ…        |
| Cloud-Ready      | âœ…        |

