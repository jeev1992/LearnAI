# Prompting Techniques

## 1. Chain of Thought (CoT) Prompting

**Chain of Thought (CoT)** is a prompting technique that **guides the model to reason step by step** before producing a final answer.  

Unlike ReAct, CoT **focuses purely on reasoning** without explicitly instructing the model to take external actions or use tools. It helps the model **break complex problems into smaller steps**, improving accuracy for multi-step tasks like math, logic, or reasoning questions.

### How it Works?
1. **Step-by-step reasoning:** The model explicitly writes out intermediate steps.  
2. **Derive the answer:** The final answer is produced after reasoning through all steps.  

**Example CoT reasoning:**

```vbnet
Question: If a train travels 60 km in 1 hour and 30 minutes, what is its average speed in km/h?

Step 1: Convert 1 hour 30 minutes into hours. 1 hour 30 minutes = 1.5 hours.
Step 2: Average speed = distance / time = 60 km / 1.5 hours.
Step 3: Average speed = 40 km/h.
Answer: 40 km/h
```

>Notice how the model explicitly lays out each step before arriving at the answer.

### Writing a CoT Prompt

A CoT prompt instructs the model to think step by step. You can provide examples of reasoning for better performance (few-shot CoT).

**Example Prompt Template:**

```vbnet
You are a helpful assistant. Solve the problem step by step and show your reasoning before giving the final answer.

Question: <your question>
Answer: Let's think step by step.
```

**Few-shot example for math:**

```vbnet
Q: What is 23 + 47?
A: Step 1: Add 20 + 40 = 60
   Step 2: Add 3 + 7 = 10
   Step 3: Add 60 + 10 = 70
   Answer: 70
```

### CoT vs ReAct

| Feature        | Chain of Thought (CoT)                    | ReAct                                         |
|----------------|------------------------------------------|-----------------------------------------------|
| **Purpose**    | Step-by-step reasoning                    | Reasoning + Acting (tool usage)             |
| **Actions**    | None                                     | Uses tools, APIs, or external info          |
| **Output**     | Answer with reasoning                     | Answer with reasoning and evidence from actions |
| **Use Case**   | Math, logic, multi-step questions        | Multi-step reasoning requiring external actions (search, database, calculators, APIs) |

### Example in LangChain

Suppose the LLM solves a multi-step math question:

```python
from langchain.llms import OpenAI

llm = OpenAI(temperature=0)

question = "If a car travels 150 km in 3 hours, what is its average speed in km/h? Answer step by step."

response = llm(question)
print(response)
```

### Sample Output (CoT style)

```vbnet
Step 1: Time traveled = 3 hours
Step 2: Distance traveled = 150 km
Step 3: Average speed = Distance / Time = 150 / 3 = 50 km/h
Answer: 50 km/h
```

> The model solves the problem by reasoning step by step before giving the final answer.

### When to Use CoT
- Math or logic problems  
- Multi-step reasoning questions  
- Complex question answering where intermediate reasoning helps accuracy  
- Any scenario where you want the model to explain its thought process  

### Summary
**Chain of Thought prompting = explicit step-by-step reasoning.**  
It’s like giving the LLM a **scratchpad** to work out the answer before responding.

---

## 2. ReAct Prompting

**ReAct** stands for **Reason + Act**. It is a prompting technique that **combines reasoning (thinking) and acting (taking actions)** in a single loop.  

Unlike Chain of Thought (CoT), which only guides the model to think step by step, **ReAct also instructs the model to take actions**, such as querying a tool, calling an API, or retrieving documents.

This is especially useful for **agentic AI**, where the model interacts with external tools or environments.

### How it Works?
1. **Thought:** The model thinks step by step about the problem.  
2. **Action:** Based on the thought, the model decides which tool or step to take.  
3. **Observation:** The model sees the result of its action.  
4. **Loop:** The model repeats Thought → Action → Observation until it reaches a final answer.

**Example Loop:**

```vbnet
Thought: I need the latest population of India.
Action: Search("India population 2025")
Observation: The population is approximately 1.42 billion.
Thought: Now I can calculate the percentage growth from last year.
Action: Calculate percentage growth using last year's data
Observation: The growth is 0.8%
Thought: Answer: India's population grew by 0.8% from last year.
```

> Notice how reasoning (Thought) and acting (Action) are interleaved.

### Writing a ReAct Prompt

A ReAct prompt tells the model to **think, act, and observe**. You can also provide instructions for tools.

**Example Prompt Template:**

```vbnet
You are an intelligent assistant that can use tools. Follow the format:

Thought: <your reasoning>
Action: <which tool or step you will take>
Observation: <result from action>
Repeat Thought → Action → Observation until you have the final answer.
Answer: <final answer>
```

### ReAct vs Chain of Thought

| Feature   | Chain of Thought                 | ReAct                                              |
|-----------|---------------------------------|---------------------------------------------------|
| Purpose   | Step-by-step reasoning           | Reasoning + Acting (tool usage)                  |
| Actions   | None                             | Uses tools, APIs, or external info               |
| Output    | Answer with reasoning            | Answer with reasoning and evidence from actions  |
| Use Case  | Math, logic, multi-step questions| Multi-step reasoning requiring external actions (search, database, calculators, APIs) |

### Example in LangChain

Suppose the LLM can search a database or call an API:

```python
from langchain.agents import Tool, initialize_agent
from langchain.llms import OpenAI

# Tool 1: Search population
def search_population(country):
    # Example function querying a database
    return f"{country} population is 1.42 billion"

# Tool 2: Calculate percentage growth
def calculate_growth(current, previous):
    growth = ((current - previous) / previous) * 100
    return f"The growth is {growth:.2f}%"

tools = [
    Tool(
        name="PopulationSearch",
        func=search_population,
        description="Get the population of a country"
    ),
    Tool(
        name="GrowthCalculator",
        func=calculate_growth,
        description="Calculate percentage growth given current and previous values"
    )
]

llm = OpenAI(temperature=0)
agent = initialize_agent(tools, llm, agent="react", verbose=True)

response = agent.run(
    "What is the population of India and its growth from last year if last year it was 1.41 billion?"
)
print(response)
```

> Here, the agent thinks about what to do, acts by calling the tool, observes the result, and finally produces an answer.

Let’s reason through the program step by step to see what the sample output would look like.

The agent has two tools:

- PopulationSearch – returns "India population is 1.42 billion"
- GrowthCalculator – computes percentage growth given current and previous values

The prompt to the agent is:

```plaintext
"What is the population of India and its growth from last year if last year it was 1.41 billion?"
```

**Step-by-step Thought → Action → Observation**

**Step 1:**

```vbnet
Thought: I need the current population of India.
Action: PopulationSearch("India")
Observation: India population is 1.42 billion
```

**Step 2:**

```vbnet
Thought: Now I can calculate the percentage growth from last year's population of 1.41 billion.
Action: GrowthCalculator(current=1.42, previous=1.41)
Observation: The growth is 0.71%
```

**Step 3:**

```vbnet
Thought: I have both the population and the growth percentage.
Answer: India's population is 1.42 billion and it grew by 0.71% from last year.
```

**Sample output:**

```plaintext
India's population is 1.42 billion and it grew by 0.71% from last year.
```

> The agent automatically selects the right tools and produces a final answer by combining their outputs.

### When to Use ReAct
- Tasks requiring **external tools or APIs**  
- Multi-step reasoning with **dynamic information**  
- Question answering that needs **retrieval or computation**  
- Building **agentic AI systems** (interactive agents)

### Summary

**ReAct prompting = interleaving reasoning and actions.**  
It’s like giving the LLM both a **brain** (thoughts) and **hands** (actions) so it can solve problems in the real world.

---

## 3. Self-Consistency Prompting

**Self-Consistency** is a prompting technique that **improves the reliability of LLM outputs by generating multiple reasoning paths** and then selecting the most consistent final answer.  

Instead of relying on a single chain of thought (CoT), the model produces **several reasoning chains**, and the answer that appears most frequently across these chains is chosen. This helps reduce mistakes in multi-step reasoning tasks.

### How it Works?
1. **Generate multiple reasoning paths:** The model is prompted to think step by step, multiple times.  
2. **Aggregate answers:** Collect all final answers from the multiple reasoning paths.  
3. **Select the most consistent answer:** The answer that appears most frequently is chosen as the final answer.

### Self-Consistency Example (Word Problem)

**Question:** If a bookstore sells 120 books in 5 days at the same rate each day, how many books do they sell per day?

**Reasoning 1:**
Step 1: Total books sold = 120  
Step 2: Total days = 5  
Step 3: Books per day = 120 ÷ 5 = 24  
Answer: 24 books per day

**Reasoning 2:**
Step 1: Average books sold per day = Total books ÷ Total days  
Step 2: 120 ÷ 5 = 24  
Answer: 24 books per day

**Reasoning 3:**
Step 1: If 5 days = 120 books, then 1 day = 120 ÷ 5  
Step 2: 120 ÷ 5 = 24  
Answer: 24 books per day

> Across all reasoning paths, the answer consistently comes out as 24. Self-consistency ensures reliability by validating the answer through multiple independent chains.

### Another Example (Logic Problem)

**Question:** If all cats are animals and some animals are black, can we conclude that some cats are black?

**Reasoning 1:**
Step 1: All cats are animals  
Step 2: Some animals are black  
Step 3: There’s no information about cats specifically being black  
Answer: Cannot conclude

**Reasoning 2:**
Step 1: Cats ⊆ Animals  
Step 2: Some Animals are black  
Step 3: Not enough info about cats themselves  
Answer: Cannot conclude

**Reasoning 3:**
Step 1: All cats belong to animals  
Step 2: Only some animals are black  
Step 3: There may or may not be black cats  
Answer: Cannot conclude

> All reasoning paths lead to the same logical conclusion: “Cannot conclude.”

### Writing a Self-Consistency Prompt

A self-consistency prompt instructs the model to generate multiple reasoning paths and optionally aggregate results:

```vbnet
Solve the following problem multiple times with independent reasoning paths. 
Then select the answer that occurs most frequently.

Question: <your question>
```

### CoT vs Self-Consistency

| Feature   | Chain of Thought                 | Self-Consistency                           |
|-----------|---------------------------------|-------------------------------------------|
| Purpose   | Step-by-step reasoning           | Reduce reasoning errors by aggregating multiple CoT outputs |
| Actions   | None                             | None (focus on reasoning consistency)    |
| Output    | Single answer with reasoning     | Most consistent answer from multiple reasoning chains |
| Use Case  | Math, logic, multi-step questions| Tasks with high uncertainty or prone to errors in reasoning |

### When to Use Self-Consistency
- Complex multi-step reasoning problems  
- Questions prone to mistakes if only a single chain is used  
- Math, logic, or science questions requiring high accuracy  

### Summary
**Self-Consistency = generating multiple CoT paths and choosing the most frequent answer.**  
It’s like giving the LLM **several scratchpads** and trusting the answer that appears most consistently across them.

---

## 4. Tree of Thoughts (ToT) Prompting

**Tree of Thoughts (ToT)** is a prompting technique that **explores multiple reasoning paths in parallel** to solve complex problems.  

Unlike CoT or Self-Consistency, which either follow a single reasoning path or aggregate multiple independent paths, ToT **organizes reasoning as a tree**, where each node represents a partial thought or step, and multiple branches explore alternative paths simultaneously. This allows the model to **plan, evaluate, and backtrack** to find the best solution.

### How it Works?
1. **Generate candidate thoughts:** At each step, the model produces several possible next thoughts.  
2. **Expand the tree:** Each candidate thought becomes a new branch for further reasoning.  
3. **Evaluate paths:** The model evaluates the potential of each path using heuristics or scoring.  
4. **Select the best path:** Continue expanding and pruning until the final answer is reached.

### Example (math problem):

```vbnet
Question: Find two numbers whose sum is 10 and product is maximized.

Step 1: Candidate thoughts for first number:
  - 1 → second number = 9 → product = 9
  - 2 → second number = 8 → product = 16
  - 3 → second number = 7 → product = 21
  - 4 → second number = 6 → product = 24
  - 5 → second number = 5 → product = 25
  - 6 → second number = 4 → product = 24
  - 7 → second number = 3 → product = 21
  - 8 → second number = 2 → product = 16
  - 9 → second number = 1 → product = 9

Step 2: Evaluate branches: Maximum product = 25

Answer: The two numbers are 5 and 5, product = 25
```
> Here, the reasoning tree explores all candidate branches and chooses the path that maximizes the product.

### Tree of Thoughts Example (Logic Puzzle)

**Question:** You have three boxes: one contains only apples, one contains only oranges, and one contains both apples and oranges. Each box is labeled incorrectly. By picking **one fruit from one box**, how can you correctly label all boxes?

**Step 1: Candidate thoughts for which box to pick from first**
- Pick from box labeled “apples and oranges”
- Pick from box labeled “apples”
- Pick from box labeled “oranges”

**Step 2: Explore branches (picking from "apples and oranges")**
- If you pick an apple → this box must be "apples only"  
- Remaining boxes:
  - Box labeled “apples” → must be "oranges only"  
  - Box labeled “oranges” → must be "apples and oranges"

- If you pick an orange → this box must be "oranges only"  
- Remaining boxes:
  - Box labeled “apples” → must be "apples and oranges"  
  - Box labeled “oranges” → must be "apples only"

**Step 3: Evaluate branches**
- Picking from the “apples and oranges” box always leads to a clear solution  
- Picking from the other boxes requires additional reasoning

**Answer:** Pick one fruit from the box labeled “apples and oranges.”  
- If it’s an apple, relabel as "apples," then fix the other two boxes accordingly.  
- If it’s an orange, relabel as "oranges," then fix the other two boxes accordingly.

> The reasoning tree explores multiple candidate choices and selects the branch that leads to the correct labeling efficiently.

### Writing a Tree of Thoughts Prompt

A ToT prompt instructs the model to think in terms of branching possibilities and evaluate multiple paths:

```vbnet
Solve the following problem by exploring multiple reasoning paths. 
At each step, generate several candidate thoughts, evaluate them, and prune less promising paths. 
Choose the final answer from the most promising path.

Question: <your question>
```

### CoT vs Self-Consistency vs Tree of Thoughts
| Feature  | Chain of Thought                  | Self-Consistency                                            | Tree of Thoughts                                                       |
| -------- | --------------------------------- | ----------------------------------------------------------- | ---------------------------------------------------------------------- |
| Purpose  | Step-by-step reasoning            | Reduce reasoning errors by aggregating multiple CoT outputs | Explore multiple reasoning paths in parallel and select the best       |
| Actions  | None                              | None                                                        | None, but evaluates alternative thought paths                          |
| Output   | Single answer with reasoning      | Most consistent answer from multiple reasoning chains       | Optimized answer based on exploration and evaluation of multiple paths |
| Use Case | Math, logic, multi-step questions | Tasks with high uncertainty or prone to errors in reasoning | Complex planning, puzzles, decision-making, or optimization problems   |

### When to Use Tree of Thoughts

- Complex multi-step reasoning problems with many possibilities
- Planning or optimization tasks
- Decision-making problems where exploring alternatives improves accuracy
- Puzzle solving and logical deduction tasks

### Summary

Tree of Thoughts = branching multiple reasoning paths and evaluating them to find the best solution.

It’s like giving the LLM a decision tree to explore multiple scratchpads simultaneously and pick the most promising path.

---

## 5. Reflexion Prompting

**Reflexion** is a prompting technique where the model **learns from its past mistakes** to improve future reasoning and outputs.  

Unlike CoT, Self-Consistency, or Tree of Thoughts, which focus on reasoning strategies during a single task, Reflexion involves **iterative self-improvement**: the model reviews previous outputs, identifies errors or suboptimal reasoning, and adjusts its approach in subsequent attempts.

### How it Works?
1. **Generate an initial answer:** The model produces a response to a question or problem.  
2. **Evaluate the answer:** The model reflects on the reasoning, identifying mistakes or weaknesses.  
3. **Revise reasoning:** The model adjusts its approach based on the reflection.  
4. **Produce improved answer:** The corrected or optimized response is generated.

### Example (math problem):

```vbnet
Question: Solve 15 × 14 step by step.

Attempt 1:
Step 1: 15 × 10 = 150
Step 2: 15 × 4 = 50
Step 3: Add 150 + 50 = 200
Answer: 200

Reflexion:
- Mistake identified: Step 2 calculation is wrong (15 × 4 = 60, not 50)

Attempt 2 (corrected):
Step 1: 15 × 10 = 150
Step 2: 15 × 4 = 60
Step 3: Add 150 + 60 = 210
Answer: 210
```

> Reflexion allows the model to detect and correct errors in its previous reasoning to produce accurate results.

### Reflexion Example (Logic Puzzle)

**Question:** There are 5 birds on a tree. A hunter shoots one. How many birds are left on the tree?

**Attempt 1:**
Step 1: Total birds = 5  
Step 2: One bird is shot → 5 − 1 = 4  
Answer: 4 birds

**Reflexion:**
- Mistake identified: When a gun is fired, the remaining birds will likely fly away. Counting only subtraction ignores this behavior.

**Attempt 2 (corrected):**
Step 1: Total birds = 5  
Step 2: One bird is shot → remaining birds likely fly away due to noise  
Answer: 0 birds

> By reflecting on the situation, the model corrects its initial simplistic reasoning and provides a more realistic answer.

### Writing a Reflexion Prompt

A Reflexion prompt encourages the model to review and critique its own reasoning:

```vbnet
Solve the following problem step by step. After generating an answer, review your steps, identify any mistakes, and produce a corrected final answer.

Question: <your question>
```

### CoT vs Self-Consistency vs Tree of Thoughts vs Reflexion
| Feature  | Chain of Thought                  | Self-Consistency                                      | Tree of Thoughts                                        | Reflexion                                               |
| -------- | --------------------------------- | ----------------------------------------------------- | ------------------------------------------------------- | ------------------------------------------------------- |
| Purpose  | Step-by-step reasoning            | Reduce errors by aggregating multiple CoT outputs     | Explore multiple reasoning paths in parallel            | Learn from past mistakes and improve reasoning          |
| Actions  | None                              | None                                                  | None                                                    | None (internal review of reasoning)                     |
| Output   | Single answer with reasoning      | Most consistent answer from multiple reasoning chains | Optimized answer based on exploration of multiple paths | Corrected answer after reflection                       |
| Use Case | Math, logic, multi-step questions | High uncertainty tasks                                | Planning, puzzles, optimization                         | Tasks prone to errors that can be improved via feedback |

### When to Use Reflexion

- Complex problems where errors are likely
- Tasks that benefit from iterative improvement
- Question answering requiring accuracy after learning from mistakes
- Educational applications where explaining corrections is helpful

### Summary

Reflexion = iterative self-review and correction.

It’s like giving the LLM a mirror to check its reasoning, learn from mistakes, and produce a better answer the next time.

---

## 6. Graph Prompting

GraphPrompting is a newer prompting technique that leverages **graph structures** to organize, reason, and explore knowledge in a structured way. Here's a detailed breakdown:

GraphPrompting is a method where the model’s reasoning or knowledge is represented as a **graph**:

- **Nodes** represent entities, concepts, or partial solutions.  
- **Edges** represent relationships, dependencies, or logical connections between nodes.  
- The model traverses, expands, or reasons over this graph to generate a final answer.  

It’s especially useful for **complex, multi-step problems** where reasoning is non-linear or has multiple paths.

### How It Works

#### 1. Problem Decomposition
- Break the problem into sub-problems or intermediate reasoning steps.  
- Each sub-problem becomes a **node** in the graph.  

#### 2. Graph Construction
- Establish **dependencies** between nodes (**edges**).  
- Some nodes may require outputs from multiple other nodes.  

#### 3. Reasoning Traversal
- The model explores the graph sequentially, in parallel, or iteratively.  
- Outputs from one node feed into connected nodes.  

#### 4. Answer Aggregation
- Once all relevant nodes are processed, the outputs are combined to produce the **final answer**.

#### Example

**Problem:** Plan a weekend trip considering budget, weather, and personal preferences.

**Nodes:**
- Budget check  
- Weather forecast  
- Transportation options  
- Accommodation options  
- Activities to do  

**Edges:**
- Budget affects accommodation and activities  
- Weather affects activities  

**Reasoning:**
1. Check budget → $500 available  
2. Check weather → sunny  
3. Suggest activities compatible with budget and weather → hiking, museum visit  
4. Suggest accommodations and transport within budget → hotel near park  

**Final output:** Complete weekend plan optimized across all constraints.

### Benefits

- Handles complex dependencies that linear prompting (like Chain-of-Thought) struggles with.  
- Can combine symbolic reasoning with LLM reasoning.  
- Easier to **debug reasoning paths** by examining the graph.

### Applications

- Multi-step math or logic problems  
- Planning & scheduling tasks  
- Scientific reasoning with dependencies  
- Knowledge retrieval and reasoning in RAG systems

### Langgraph Implementation

```python
# Install LangGraph
# pip install langgraph openai

from langgraph import Graph, Node, Edge
from langchain.chat_models import ChatOpenAI

# Initialize LLM
llm = ChatOpenAI(model_name="gpt-4", temperature=0)

# Step 1: Define nodes
budget_node = Node("Check budget for weekend trip")
weather_node = Node("Check weather forecast for destination")
transport_node = Node("Determine transportation options within budget")
accommodation_node = Node("Determine accommodation options within budget")
activities_node = Node("Suggest activities compatible with weather and budget")
final_plan_node = Node("Aggregate final weekend plan")

# Step 2: Define graph
graph = Graph()

# Add nodes
graph.add_nodes([budget_node, weather_node, transport_node, accommodation_node, activities_node, final_plan_node])

# Step 3: Define edges (dependencies)
graph.add_edges([
    Edge(budget_node, accommodation_node),
    Edge(budget_node, activities_node),
    Edge(weather_node, activities_node),
    Edge(accommodation_node, final_plan_node),
    Edge(activities_node, final_plan_node),
    Edge(transport_node, final_plan_node)
])

# Step 4: Define reasoning function for each node
def reasoning(node_name, context={}):
    prompt = f"Node: {node_name}\nContext: {context}\nProvide detailed reasoning."
    return llm.predict(prompt)

# Step 5: Traverse graph and store results
results = {}
for node in graph.topological_sort():  # Ensure dependencies are respected
    # Collect context from parent nodes
    parent_context = {parent.name: results[parent.name] for parent in graph.get_parents(node)}
    results[node.name] = reasoning(node.name, context=parent_context)

# Step 6: View final weekend plan
print("=== Final Weekend Plan ===")
print(results[final_plan_node.name])
```