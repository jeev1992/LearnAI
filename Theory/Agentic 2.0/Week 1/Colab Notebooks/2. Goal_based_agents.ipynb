{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Goal-based agents\n",
        "\n",
        "This agent uses simple coordinate math to formulate its plan. It looks at the goal coordinates, calculates how many steps \"Down\" and \"Right\" it needs, creates a full list of actions (the plan), and then executes them.\n",
        "\n",
        "This creates the simplest distinction between a Reflex Agent and a Goal-Based Agent:\n",
        "\n",
        "**Reflex Agent**: \"I see I'm not at the goal. I'll take one step Right.\" (Repeat 100 times).\n",
        "\n",
        "**Goal-Based**: \"To get to (4,4), I need 4 Down moves and 4 Right moves. Here is my list. Go.\""
      ],
      "metadata": {
        "id": "0bo6BFfMMgm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "\n",
        "class Environment:\n",
        "    \"\"\"\n",
        "    Represents a simple 2D Grid.\n",
        "    Agent 'A' needs to get to Goal 'G'.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, start, goal):\n",
        "        self.size = size\n",
        "        self.agent_pos = list(start) # Mutable list for easier updates\n",
        "        self.goal_pos = list(goal)\n",
        "        self.path_taken = []\n",
        "\n",
        "    def update_agent_pos(self, action):\n",
        "        \"\"\"Updates the internal state based on the agent's action.\"\"\"\n",
        "        if action == \"Down\":\n",
        "            self.agent_pos[0] += 1\n",
        "        elif action == \"Up\":\n",
        "            self.agent_pos[0] -= 1\n",
        "        elif action == \"Right\":\n",
        "            self.agent_pos[1] += 1\n",
        "        elif action == \"Left\":\n",
        "            self.agent_pos[1] -= 1\n",
        "\n",
        "        # Store as tuple for history\n",
        "        self.path_taken.append(tuple(self.agent_pos))\n",
        "\n",
        "    def display(self):\n",
        "        \"\"\"Prints the grid to the console.\"\"\"\n",
        "        # clear screen command (cls for windows, clear for unix)\n",
        "        os.system('cls' if os.name == 'nt' else 'clear')\n",
        "\n",
        "        print(f\"--- SIMPLE GOAL AGENT ---\")\n",
        "        print(f\"Goal: {self.goal_pos} | Current: {self.agent_pos}\")\n",
        "        print(\"-\" * (self.size * 3 + 1))\n",
        "\n",
        "        for r in range(self.size):\n",
        "            row_str = \"|\"\n",
        "            for c in range(self.size):\n",
        "                if r == self.agent_pos[0] and c == self.agent_pos[1]:\n",
        "                    row_str += \" A \"\n",
        "                elif r == self.goal_pos[0] and c == self.goal_pos[1]:\n",
        "                    row_str += \" G \"\n",
        "                elif (r,c) in self.path_taken:\n",
        "                     row_str += \" . \"\n",
        "                else:\n",
        "                    row_str += \"   \"\n",
        "            print(row_str + \"|\")\n",
        "        print(\"-\" * (self.size * 3 + 1))\n",
        "        time.sleep(0.8)"
      ],
      "metadata": {
        "id": "8xTLSebJMIIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GoalBasedAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.plan = []\n",
        "\n",
        "    def perceive(self):\n",
        "        return self.env.agent_pos\n",
        "\n",
        "    def formulate_goal(self):\n",
        "        return self.env.goal_pos\n",
        "\n",
        "    def formulate_plan(self, start, goal):\n",
        "        \"\"\"\n",
        "        Simple Planner (No BFS/DFS)\n",
        "        Instead of searching a graph, we just calculate the arithmetic difference\n",
        "        to generate a plan sequence.\n",
        "        \"\"\"\n",
        "        print(\"Agent is formulating a plan based on coordinates...\")\n",
        "        time.sleep(1)\n",
        "\n",
        "        plan = []\n",
        "\n",
        "        # 1. Calculate vertical distance\n",
        "        diff_row = goal[0] - start[0]\n",
        "        if diff_row > 0:\n",
        "            plan.extend([\"Down\"] * diff_row)\n",
        "        elif diff_row < 0:\n",
        "            plan.extend([\"Up\"] * abs(diff_row))\n",
        "\n",
        "        # 2. Calculate horizontal distance\n",
        "        diff_col = goal[1] - start[1]\n",
        "        if diff_col > 0:\n",
        "            plan.extend([\"Right\"] * diff_col)\n",
        "        elif diff_col < 0:\n",
        "            plan.extend([\"Left\"] * abs(diff_col))\n",
        "\n",
        "        return plan\n",
        "\n",
        "    def run(self):\n",
        "        # 1. PERCEIVE\n",
        "        current_loc = self.perceive()\n",
        "        goal_loc = self.formulate_goal()\n",
        "\n",
        "        # 2. PLAN\n",
        "        # The key difference: We generate the FULL plan before taking a single step.\n",
        "        self.plan = self.formulate_plan(current_loc, goal_loc)\n",
        "\n",
        "        print(f\"Plan created: {self.plan}\")\n",
        "        time.sleep(2)\n",
        "\n",
        "        # 3. ACT\n",
        "        # Execute the plan blindly (assuming the world doesn't change)\n",
        "        for action in self.plan:\n",
        "            self.env.update_agent_pos(action)\n",
        "            self.env.display()\n",
        "\n",
        "        print(\"Goal Reached!\")\n"
      ],
      "metadata": {
        "id": "-RjvADAkM4zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "world = Environment(5, (0, 0), (4, 4))\n",
        "agent = GoalBasedAgent(world)\n",
        "\n",
        "world.display()\n",
        "agent.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMxKofZmMRE2",
        "outputId": "eaafa2f0-1606-4e12-867f-f9adf97bf657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- SIMPLE GOAL AGENT ---\n",
            "Goal: [4, 4] | Current: [0, 0]\n",
            "----------------\n",
            "| A             |\n",
            "|               |\n",
            "|               |\n",
            "|               |\n",
            "|             G |\n",
            "----------------\n",
            "Agent is formulating a plan based on coordinates...\n",
            "Plan created: ['Down', 'Down', 'Down', 'Down', 'Right', 'Right', 'Right', 'Right']\n",
            "--- SIMPLE GOAL AGENT ---\n",
            "Goal: [4, 4] | Current: [1, 0]\n",
            "----------------\n",
            "|               |\n",
            "| A             |\n",
            "|               |\n",
            "|               |\n",
            "|             G |\n",
            "----------------\n",
            "--- SIMPLE GOAL AGENT ---\n",
            "Goal: [4, 4] | Current: [2, 0]\n",
            "----------------\n",
            "|               |\n",
            "| .             |\n",
            "| A             |\n",
            "|               |\n",
            "|             G |\n",
            "----------------\n",
            "--- SIMPLE GOAL AGENT ---\n",
            "Goal: [4, 4] | Current: [3, 0]\n",
            "----------------\n",
            "|               |\n",
            "| .             |\n",
            "| .             |\n",
            "| A             |\n",
            "|             G |\n",
            "----------------\n",
            "--- SIMPLE GOAL AGENT ---\n",
            "Goal: [4, 4] | Current: [4, 0]\n",
            "----------------\n",
            "|               |\n",
            "| .             |\n",
            "| .             |\n",
            "| .             |\n",
            "| A           G |\n",
            "----------------\n",
            "--- SIMPLE GOAL AGENT ---\n",
            "Goal: [4, 4] | Current: [4, 1]\n",
            "----------------\n",
            "|               |\n",
            "| .             |\n",
            "| .             |\n",
            "| .             |\n",
            "| .  A        G |\n",
            "----------------\n",
            "--- SIMPLE GOAL AGENT ---\n",
            "Goal: [4, 4] | Current: [4, 2]\n",
            "----------------\n",
            "|               |\n",
            "| .             |\n",
            "| .             |\n",
            "| .             |\n",
            "| .  .  A     G |\n",
            "----------------\n",
            "--- SIMPLE GOAL AGENT ---\n",
            "Goal: [4, 4] | Current: [4, 3]\n",
            "----------------\n",
            "|               |\n",
            "| .             |\n",
            "| .             |\n",
            "| .             |\n",
            "| .  .  .  A  G |\n",
            "----------------\n",
            "--- SIMPLE GOAL AGENT ---\n",
            "Goal: [4, 4] | Current: [4, 4]\n",
            "----------------\n",
            "|               |\n",
            "| .             |\n",
            "| .             |\n",
            "| .             |\n",
            "| .  .  .  .  A |\n",
            "----------------\n",
            "Goal Reached!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the key concepts demonstrated in this specific implementation of a Goal-Based Agent:\n",
        "\n",
        "1. Planning Before Acting (The Core Difference): Unlike a reflex agent that acts on impulse (e.g., \"I see a wall, I turn left\"), this agent uses formulate_plan to generate a complete sequence of actions (['Down', 'Down', ..., 'Right', 'Right']) before it ever moves its motors. It \"thinks\" through the entire problem first.\n",
        "\n",
        "2. Explicit Goal Formulation: The agent calls formulate_goal() to define exactly what success looks like ((4,4)). All future actions are derived specifically to satisfy this target state.\n",
        "\n",
        "3. Open-Loop Execution: In the run() method, once the plan is made, the agent executes it blindly:\n",
        "```python\n",
        "for action in self.plan:\n",
        "    self.env.update_agent_pos(action)\n",
        "```\n",
        "This is known as \"Open-Loop\" control. The agent assumes the world won't change while it is moving. If an obstacle were suddenly dropped in its path after it started moving, this specific agent would crash because it isn't checking its sensors during the execution phase.\n",
        "\n",
        "4. World Representation (Coordinates): The agent understands the world through coordinates. Its \"intelligence\" here is simple arithmetic: calculating the vertical difference (goal[0] - start[0]) and horizontal difference (goal[1] - start[1]) to determine the necessary steps. This is a very basic form of a heuristic"
      ],
      "metadata": {
        "id": "cMCq-p2hNVi7"
      }
    }
  ]
}