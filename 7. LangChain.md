
# üß† Top 5 Features of LangChain (with Code + Use Cases)

---

## ‚úÖ 1. Prompt Templates

### üîç What it does:
Helps you **create dynamic and reusable prompts** with variable placeholders.

### üìå Use Case:
You‚Äôre building a **customer service chatbot** that tailors its response to a specific product and tone.

```python
from langchain.prompts import PromptTemplate

template = PromptTemplate(
    input_variables=["product", "tone"],
    template="Explain how to use {product} in a {tone} tone."
)

prompt = template.format(product="washing machine", tone="friendly")
print(prompt)
```

üßæ Output:
```
Explain how to use washing machine in a friendly tone.
```

---

## ‚úÖ 2. Chains (LLMChain, SequentialChain)

### üîç What it does:
Chains connect **LLMs + prompts + output parsers + tools** into pipelines.

### üìå Use Case:
Build a **two-step system**: Generate a blog title ‚Üí then expand into full content.

```python
from langchain.chains import LLMChain, SequentialChain
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate

llm = ChatOpenAI(temperature=0.7)

title_prompt = PromptTemplate.from_template("Write a catchy blog title about {topic}.")
title_chain = LLMChain(llm=llm, prompt=title_prompt, output_key="title")

content_prompt = PromptTemplate.from_template("Write a blog post titled '{title}'.")
content_chain = LLMChain(llm=llm, prompt=content_prompt, output_key="blog")

full_chain = SequentialChain(
    chains=[title_chain, content_chain],
    input_variables=["topic"],
    output_variables=["title", "blog"]
)

output = full_chain({"topic": "AI in Healthcare"})
print(output["blog"])
```

---

## ‚úÖ 3. Retrievers + RAG (Retrieval-Augmented Generation)

### üîç What it does:
Connects your **documents or knowledge base** to LLMs via embedding search.

### üìå Use Case:
‚ÄúChat with PDF‚Äù or ‚ÄúAsk your company‚Äôs documentation‚Äù

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_loaders import TextLoader

docs = TextLoader("faq.txt").load()
splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunks = splitter.split_documents(docs)

vectorstore = FAISS.from_documents(chunks, OpenAIEmbeddings())
retriever = vectorstore.as_retriever()

qa_chain = RetrievalQA.from_chain_type(llm=ChatOpenAI(), retriever=retriever)
response = qa_chain.run("What is your refund policy?")
print(response)
```

---

## ‚úÖ 4. Tools & Agents

### üîç What it does:
Lets LLMs **use external tools** (like Python, web search, or APIs) in an autonomous fashion.

### üìå Use Case:
Build a **travel assistant** that fetches weather and calculates expenses dynamically.

```python
from langchain.agents import initialize_agent, Tool
from langchain.agents.agent_types import AgentType
from langchain.utilities import PythonREPL
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI()

tools = [
    Tool(
        name="Python Calculator",
        func=PythonREPL().run,
        description="Useful for solving math or date problems"
    )
]

agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

response = agent.run("What is 1.15 times the 2024 inflation-adjusted price of $80?")
print(response)
```

---
### üîß How LangChain Agents Know When to Use Tools

LangChain agents use the **ReAct reasoning paradigm** to let the LLM decide **when and how to use tools**. Here's how it works:

---

#### üß† Step-by-Step: How the LLM Decides to Use Tools

#### 1. üîß Tool Descriptions Are Passed as Part of the Prompt

When you define a tool in LangChain:

```python
from langchain.utilities import PythonREPL
from langchain.agents import Tool

tool = Tool(
    name="Python Calculator",
    func=PythonREPL().run,
    description="Useful for solving math or date problems"
)
```

LangChain includes this in the LLM's system prompt, so the model knows:

> ‚ÄúThere‚Äôs a tool called `Python Calculator`. You can use it if math or date calculations come up.‚Äù

---

#### 2. üßæ LangChain Formats the Prompt Using ReAct Framework

The agent uses a ReAct-style template like:

```
You are a helpful assistant. You have access to the following tools:

Python Calculator: Useful for solving math or date problems.

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [Python Calculator]
Action Input: the input to the action
Observation: the result of the action
... (this can repeat) ...
Final Answer: the answer to the original question
```

---

#### 3. ü§ñ LLM Follows This Format to Reason

For a user query like:

> What is 1.15 times the 2024 inflation-adjusted price of $80?

The LLM responds like this:

```
Question: What is 1.15 times the 2024 inflation-adjusted price of $80?
Thought: I need to multiply 80 by 1.15. I should use the Python Calculator.
Action: Python Calculator
Action Input: 80 * 1.15
```

LangChain:
- Executes the tool
- Captures the result (`92.0`)
- Feeds it back to the LLM

Then the LLM completes the reasoning:

```
Thought: Now I know the result is 92.0
Final Answer: The inflation-adjusted price is $92.0
```

---

#### üîÑ It's a Loop: LangChain Acts as the Brainstem

1. LLM outputs an action + input
2. LangChain runs the tool
3. Feeds result back to LLM
4. LLM decides next step or gives Final Answer

---

#### ‚úÖ Summary: Why This Works

| Mechanism              | Purpose                                                |
|------------------------|--------------------------------------------------------|
| Tool descriptions      | Tell LLM what tools are available                      |
| ReAct prompting        | Gives LLM a format to reason and choose tools          |
| LangChain agent loop   | Parses LLM output and manages execution flow           |
| LLM‚Äôs language skill   | Recognizes context and when a tool is more appropriate |

---

#### üß™ Example Output (Real Trace)

```
Question: What is 1.15 * 80?
Thought: I should calculate 1.15 * 80 using the calculator.
Action: Python Calculator
Action Input: 1.15 * 80
Observation: 92.0
Thought: Now that I have the result, I can answer the question.
Final Answer: The result is 92.0.
```


## ‚úÖ 5. Memory (Conversation Buffer, Summary, Entity)

### üîç What it does:
Enables **stateful chat applications** by letting the LLM remember previous inputs.

### üìå Use Case:
Chatbot that remembers your name and context across multiple turns.

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain.chat_models import ChatOpenAI

memory = ConversationBufferMemory()
chat_chain = ConversationChain(llm=ChatOpenAI(), memory=memory, verbose=True)

print(chat_chain.run("My name is Jeevendra."))
print(chat_chain.run("What's my name?"))
```

üßæ Output:
```
Your name is Jeevendra.
```

---

### ‚úÖ Memory in LangChain: Conversation Buffer, Summary, Entity

LangChain provides several types of memory mechanisms that allow **stateful chat applications**. These memories enable an LLM to **remember previous inputs**, creating interactive and context-aware applications.

---

#### üîç What It Does

By default, LLMs are **stateless** ‚Äî they don't remember previous interactions unless you include them in the input prompt. LangChain memory classes **store conversation history** and inject them into prompts automatically.

---

#### üìö Types of Memory

| Memory Type                      | Description                                                               |
|----------------------------------|---------------------------------------------------------------------------|
| `ConversationBufferMemory`       | Stores **entire conversation** verbatim, like a running chat log.         |
| `ConversationBufferWindowMemory`| Stores only the **last N messages**.                                      |
| `ConversationSummaryMemory`     | Stores a **summary of past interactions** to save token space.            |
| `ConversationEntityMemory`      | Tracks and remembers **entities** (e.g., names, places) across messages.  |
| `CombinedMemory`                | Combines multiple memory types.                                           |

---

#### ‚úÖ `ConversationBufferMemory` (Most Basic)

### What it does:
Stores the **entire chat history** and appends it to each new user message. This allows the LLM to maintain continuity in the conversation.

---

#### üìå Use Case

Create a chatbot that remembers your name across multiple user inputs.

#### ‚úÖ Code Example

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain.chat_models import ChatOpenAI

# Initialize memory
memory = ConversationBufferMemory()

# Create conversation chain
chat_chain = ConversationChain(
    llm=ChatOpenAI(temperature=0),
    memory=memory,
    verbose=True
)

# Provide context
print(chat_chain.run("My name is Jeevendra."))

# Ask a follow-up
print(chat_chain.run("What's my name?"))
```

#### üßæ Output:
```
Your name is Jeevendra.
```

---

#### üß† How It Works Internally

The chain maintains a buffer like:

```
Human: My name is Jeevendra.
AI: Nice to meet you, Jeevendra!
Human: What's my name?
```

This entire history is passed along with every prompt, giving the LLM full context.

---

#### ‚öñÔ∏è Trade-offs

| Pros                                | Cons                                   |
|-------------------------------------|----------------------------------------|
| Simple to use                       | Token count grows with each message    |
| Easy debugging via `verbose=True`   | Inefficient for long conversations     |
| Useful for personal or test bots    | Can cause latency as context grows     |

---

#### üì¶ Advanced Variants

#### 1. **ConversationSummaryMemory**
Summarizes the conversation history using the LLM:
```python
from langchain.memory import ConversationSummaryMemory
memory = ConversationSummaryMemory(llm=ChatOpenAI(), return_messages=True)
```

#### 2. **ConversationEntityMemory**
Tracks named entities (like name, city, etc.):
```python
from langchain.memory import ConversationEntityMemory
memory = ConversationEntityMemory(llm=ChatOpenAI(), return_messages=True)
```

---

#### üõ†Ô∏è Real-World Use Cases

| Application            | Description                                                   |
|------------------------|---------------------------------------------------------------|
| Customer Support Bot   | Remembers user name, account info, and issue context          |
| Personal Assistant     | Retains user preferences, appointments, and tasks             |
| Interview Simulator    | Adjusts questions based on remembered past answers            |
| Storytelling Agent     | Remembers characters and events in a story                    |
| Language Tutor         | Tracks user progress and feedback                             |

---

#### üß™ Debug Tip

See what is stored in the memory:

```python
print(memory.buffer)
```

Example output:
```
Human: My name is Jeevendra.
AI: Nice to meet you, Jeevendra!
Human: I live in Bangalore.
AI: Great, Bangalore is a lovely city.
```

---

#### üßæ Sample Flow

```python
chat_chain.run("My name is Jeevendra.")
# ‚Üí Nice to meet you, Jeevendra!

chat_chain.run("I live in Bangalore.")
# ‚Üí Great, Bangalore is a lovely city.

chat_chain.run("Where do I live?")
# ‚Üí You said you live in Bangalore.
```

---

## üìä Summary Table

| Feature        | Use Case                                | Benefit                                  |
|----------------|------------------------------------------|-------------------------------------------|
| Prompt Templates | Customer bots, automation               | Reusable, dynamic language scaffolding    |
| Chains         | Blog writing, interview prep, pipelines  | Multi-step generation & orchestration     |
| RAG + Retriever| Chat with docs, internal search          | Accurate LLM answers with local data      |
| Tools + Agents | Travel planner, math assistant           | LLMs act with APIs and code               |
| Memory         | Stateful chat, persona bots              | Context continuity over long conversations|

---


